{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f3b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, QuantileTransformer, PowerTransformer, Normalizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, accuracy_score,\n",
    "    roc_auc_score, f1_score, log_loss, brier_score_loss, average_precision_score,  \n",
    "    balanced_accuracy_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras.ops as ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f47903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - LOADING DATA\n",
    "separator = ','\n",
    "fold = \"4\" # set the fold number for the files\n",
    "\n",
    "url_train = \"data/6h/train_df_kfold_\"+fold+\".csv\"\n",
    "url_val = \"data/6h/val_df_kfold_\"+fold+\".csv\"\n",
    "url_test = \"data/6h/test_df_kfold_\"+fold+\".csv\"\n",
    "\n",
    "\n",
    "# CHOOSE COLUMNS TO DELETE\n",
    "\n",
    "qtde_attributes = 18 #18 10\n",
    "\n",
    "#18 attributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class', 'T_REC', 'harpnum']\n",
    "\n",
    "#10 Bobra'attributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class','T_REC', 'harpnum', 'MEANGAM','MEANGBH','MEANGBT','MEANGBZ', 'MEANJZD','MEANJZH','MEANALP','MEANSHR']\n",
    "\n",
    "#10 SHAP'atrributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class', 'T_REC', 'harpnum' ,'ABSNJZH','MEANGAM','MEANJZD','MEANJZH','SAVNCPP','TOTPOT','TOTUSJH','TOTUSJZ']\n",
    "\n",
    "\n",
    "# NORMALIZATION\n",
    "scaler_name = 'StandardScaler'  #'StandardScaler', 'RobustScaler', 'MinMaxScaler', 'NormalizerL1', 'PowerTransformer', 'QuantileTransformer'\n",
    "\n",
    "\n",
    "# BALANCING\n",
    "balanceamento = 'smote'  #'smote', 'oversampling', 'undersampling', 'class_weight', ou 'none'\n",
    "\n",
    "# Feature Transformer (FT) model configuration\n",
    "embed_dim=128\n",
    "num_heads=8\n",
    "ff_dim=128\n",
    "num_transformer_blocks=6\n",
    "dropout_rate=0.3\n",
    "\n",
    "epoch = 100 \n",
    "batch = 64 \n",
    "\n",
    "optimizer=keras.optimizers.Adam(learning_rate=1e-4) #1e4\n",
    "focal_gamma = 2\n",
    "focal_alpha = 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e13d4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_31300\\1189905242.py:3: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(url_train, sep=separator)\n",
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_31300\\1189905242.py:4: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(url_val, sep=separator)\n",
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_31300\\1189905242.py:5: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(url_test, sep=separator)\n"
     ]
    }
   ],
   "source": [
    "# 3 - READ AND PREPARE DATA\n",
    "\n",
    "train_df = pd.read_csv(url_train, sep=separator)\n",
    "val_df = pd.read_csv(url_val, sep=separator)\n",
    "test_df = pd.read_csv(url_test, sep=separator)\n",
    "\n",
    "\n",
    "# Convert datetime\n",
    "date1_ta = pd.to_datetime(train_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_ta = pd.to_datetime(train_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "date1_va = pd.to_datetime(val_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_va = pd.to_datetime(val_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "date1_te = pd.to_datetime(test_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_te = pd.to_datetime(test_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "train_df['T_REC'] = date1_ta.fillna(date2_ta)\n",
    "val_df['T_REC'] = date1_va.fillna(date2_va)\n",
    "test_df['T_REC'] = date1_te.fillna(date2_te)\n",
    "\n",
    "# Remove timezone to avoid date shifts\n",
    "train_df['T_REC'] = train_df['T_REC'].dt.tz_localize(None)\n",
    "val_df['T_REC'] = val_df['T_REC'].dt.tz_localize(None)\n",
    "test_df['T_REC'] = test_df['T_REC'].dt.tz_localize(None)\n",
    "\n",
    "# order date\n",
    "train_df = train_df.sort_values(by='T_REC')\n",
    "val_df = val_df.sort_values(by='T_REC')\n",
    "test_df = test_df.sort_values(by='T_REC')\n",
    "\n",
    "\n",
    "#save test extra columns\n",
    "harpnum_test = test_df['harpnum'].values\n",
    "t_rec_test = test_df['T_REC'].values\n",
    "letra_class_test = test_df['Letra_Class'].values\n",
    "\n",
    "info_test = pd.DataFrame({\n",
    "    'harpnum': harpnum_test,\n",
    "    'T_REC': t_rec_test,\n",
    "    'Letra_Class': letra_class_test\n",
    "})\n",
    "info_test.to_csv('results/ft-transformers-info_test-fold'+fold+'.csv', index=False)\n",
    "\n",
    "\n",
    "#delete columns\n",
    "for lcd in list_col_delete:\n",
    "    train_df.pop(lcd)\n",
    "    val_df.pop(lcd)\n",
    "    test_df.pop(lcd)\n",
    "    \n",
    "\n",
    "target_col = 'Class'\n",
    "X_train = train_df.drop(columns=[target_col]).values\n",
    "y_train = train_df[target_col].values\n",
    "X_val = val_df.drop(columns=[target_col]).values\n",
    "y_val = val_df[target_col].values\n",
    "X_test = test_df.drop(columns=[target_col]).values\n",
    "y_test = test_df[target_col].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec9c79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 - NORMALIZATION AND BALANCING\n",
    "\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_robust = RobustScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_l1 = Normalizer(norm='l1')\n",
    "transformer_yeo = PowerTransformer(method='yeo-johnson')\n",
    "transformer_quantile = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Choose the scaler based on the scaler_name variable\n",
    "if scaler_name == 'StandardScaler':\n",
    "    scaler = scaler_standard\n",
    "elif scaler_name == 'RobustScaler':\n",
    "    scaler = scaler_robust\n",
    "elif scaler_name == 'MinMaxScaler':\n",
    "    scaler = scaler_minmax\n",
    "elif scaler_name == 'NormalizerL1':\n",
    "    scaler = scaler_l1\n",
    "elif scaler_name == 'PowerTransformer':\n",
    "    scaler = transformer_yeo\n",
    "elif scaler_name == 'QuantileTransformer':\n",
    "    scaler = transformer_quantile\n",
    "else:\n",
    "    raise ValueError(f\"Scaler '{scaler_name}' não reconhecido. Escolha um válido.\")\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "if balanceamento == 'smote':\n",
    "    smote = SMOTE(sampling_strategy=0.6, k_neighbors=3, random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "    class_weights = None  # not necessary in this case\n",
    "\n",
    "elif balanceamento == 'oversampling':\n",
    "    oversample = RandomOverSampler(sampling_strategy=0.6, random_state=42)\n",
    "    X_train_res, y_train_res = oversample.fit_resample(X_train, y_train)\n",
    "    class_weights = None\n",
    "\n",
    "elif balanceamento == 'undersampling':\n",
    "    undersample = RandomUnderSampler(sampling_strategy=0.6, random_state=42)\n",
    "    X_train_res, y_train_res = undersample.fit_resample(X_train, y_train)\n",
    "    class_weights = None\n",
    "\n",
    "elif balanceamento == 'class_weight':\n",
    "    # Don't change X_train/y_train, just calculate the weights\n",
    "    X_train_res, y_train_res = X_train, y_train\n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights = dict(zip(classes, weights))\n",
    "\n",
    "elif balanceamento == 'none':\n",
    "    X_train_res, y_train_res = X_train, y_train\n",
    "    class_weights = None\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid balancing method. Choose from: 'smote', 'oversample', 'undersample', 'class_weight', 'none'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4973f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1722/1722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m903s\u001b[0m 516ms/step - auc: 0.7245 - fn: 2917.0000 - fp: 1204.0000 - loss: 0.0114 - precision: 0.0452 - recall: 0.0192 - tn: 436630.0000 - tp: 57.0000 - val_auc: 0.9290 - val_fn: 857.0000 - val_fp: 0.0000e+00 - val_loss: 0.0061 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_tn: 107907.0000 - val_tp: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#5- FT-TRANSFORMEFS MODEL\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "\n",
    "class AddCLSToken(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.cls_token = self.add_weight(\n",
    "            shape=(1, 1, self.embed_dim),\n",
    "            initializer=tf.keras.initializers.RandomNormal(),\n",
    "            trainable=True,\n",
    "            name=\"cls_token\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = ops.shape(x)[0]\n",
    "        cls_tokens = ops.tile(self.cls_token, [batch_size, 1, 1])\n",
    "        return ops.concatenate([cls_tokens, x], axis=1)\n",
    "\n",
    "\n",
    "def build_ft_transformer(\n",
    "    n_features,\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    ff_dim=128,\n",
    "    num_transformer_blocks=2,\n",
    "    dropout_rate=0.1\n",
    "):\n",
    "    inputs = keras.Input(shape=(n_features,))\n",
    "    \n",
    "    # Tokenization of numeric tabular data\n",
    "    x = NumericalFeatureTokenizer(n_features, embed_dim)(inputs)\n",
    "\n",
    "    # Add learnable [CLS] token\n",
    "    x = AddCLSToken(embed_dim)(x)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # Multi-head self-attention + residual\n",
    "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x_norm, x_norm)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "\n",
    "        # Feed-forward + residual\n",
    "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        ff_output = layers.Dense(ff_dim, activation=\"gelu\")(x_norm)\n",
    "        ff_output = layers.Dense(embed_dim)(ff_output)\n",
    "        x = layers.Add()([x, ff_output])\n",
    "\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Token output [CLS]\n",
    "    cls_output = x[:, 0, :]  # (batch_size, embed_dim)\n",
    "\n",
    "    # Final classification layer\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(cls_output)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ft-transformer customer for tabular data\n",
    "def build_ft_transformer_old2(\n",
    "    n_features, \n",
    "    embed_dim=64, \n",
    "    num_heads=8, \n",
    "    ff_dim=128, \n",
    "    num_transformer_blocks=2, \n",
    "    dropout_rate=0.1\n",
    "):\n",
    "    inputs = keras.Input(shape=(n_features,))\n",
    "\n",
    "    # (batch_size, n_features, 1)\n",
    "    x = layers.Reshape((n_features, 1))(inputs)\n",
    "\n",
    "    # Embedding de cada feature\n",
    "    x = layers.TimeDistributed(layers.Dense(embed_dim))(x)\n",
    "\n",
    "    # --- Adiciona o CLS token ---\n",
    "    cls_token = tf.Variable(initial_value=tf.random.normal([1, 1, embed_dim]), trainable=True, name=\"cls_token\")\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    cls_tokens = tf.tile(cls_token, [batch_size, 1, 1])  # (batch_size, 1, embed_dim)\n",
    "    x = tf.concat([cls_tokens, x], axis=1)  # (batch_size, n_features+1, embed_dim)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # LayerNorm antes da Attention (Pre-LN)\n",
    "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x_norm, x_norm)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "\n",
    "        # Feedforward\n",
    "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        ff_output = layers.Dense(ff_dim, activation=\"relu\")(x_norm)\n",
    "        ff_output = layers.Dense(embed_dim)(ff_output)\n",
    "        x = layers.Add()([x, ff_output])\n",
    "        \n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # --- Só usa o CLS token ---\n",
    "    cls_output = x[:, 0, :]  # Pega só o primeiro token (CLS)\n",
    "\n",
    "    # Classificador final\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(cls_output)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class NumericalFeatureTokenizer(layers.Layer):\n",
    "    def __init__(self, n_features, embed_dim):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.embed_dim = embed_dim\n",
    "        # Um peso aprendível para cada feature\n",
    "        self.linear_weights = self.add_weight(\n",
    "            shape=(n_features, embed_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name=\"feature_embedding_weights\"\n",
    "        )\n",
    "        # Um bias aprendível por feature\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(n_features, embed_dim),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name=\"feature_embedding_bias\"\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch_size, n_features)\n",
    "        # Applies a linear transformation per feature\n",
    "        x = tf.expand_dims(inputs, -1)  # (batch_size, n_features, 1)\n",
    "        out = x * self.linear_weights + self.bias  # broadcasting\n",
    "        return out  # (batch_size, n_features, embed_dim)\n",
    "\n",
    "#ft-transformer - papper\n",
    "def build_ft_transformer_old(\n",
    "    n_features,\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    ff_dim=128,\n",
    "    num_transformer_blocks=2,\n",
    "    dropout_rate=0.1\n",
    "):\n",
    "    inputs = keras.Input(shape=(n_features,))\n",
    "    \n",
    "    # Tokenization of numeric tabular data\n",
    "    x = NumericalFeatureTokenizer(n_features, embed_dim)(inputs)\n",
    "\n",
    "    # Add learnable [CLS] token\n",
    "    cls_token = tf.Variable(\n",
    "        initial_value=tf.random.normal([1, 1, embed_dim]),\n",
    "        trainable=True,\n",
    "        name=\"cls_token\"\n",
    "    )\n",
    "    batch_size = ops.shape(x)[0]\n",
    "    cls_tokens = ops.tile(cls_token, [batch_size, 1, 1])\n",
    "    x = ops.concat([cls_tokens, x], axis=1)  # (batch_size, n_features+1, embed_dim)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # Multi-head self-attention + residual\n",
    "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x_norm, x_norm)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "\n",
    "        # Feed-forward + residual\n",
    "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        ff_output = layers.Dense(ff_dim, activation=\"gelu\")(x_norm)\n",
    "        ff_output = layers.Dense(embed_dim)(ff_output)\n",
    "        x = layers.Add()([x, ff_output])\n",
    "\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Token output [CLS]\n",
    "    cls_output = x[:, 0, :]  # (batch_size, embed_dim)\n",
    "\n",
    "    # Final classification layer\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(cls_output)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        modulating_factor = tf.pow((1 - p_t), gamma)\n",
    "        return tf.reduce_mean(alpha_factor * modulating_factor * bce)\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Create model ---\n",
    "model = build_ft_transformer(\n",
    "    n_features=n_features,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_transformer_blocks=num_transformer_blocks,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#compile model \n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=binary_focal_loss(gamma=focal_gamma, alpha=focal_alpha),\n",
    "    metrics=[\n",
    "        AUC(name='auc'),\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "        TruePositives(name='tp'),\n",
    "        TrueNegatives(name='tn'),\n",
    "        FalsePositives(name='fp'),\n",
    "        FalseNegatives(name='fn')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csv_logger = CSVLogger('results/ft-transformers-trainning_log_fold_'+fold+'.csv', append=True)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', patience=10, mode='max', restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.keras', monitor='val_auc', mode='max', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, mode='max', verbose=1),\n",
    "    csv_logger\n",
    "]\n",
    "\n",
    "\n",
    "#Fit Model\n",
    "\n",
    "if class_weights == None:\n",
    "    history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epoch,\n",
    "    batch_size=batch,\n",
    "    verbose=1\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epoch,\n",
    "    batch_size=batch,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4900e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 - Salve training e val metrics\n",
    "\n",
    "\n",
    "# Convert history to DataFrame\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "\n",
    "\n",
    "# Save to a CSV file\n",
    "history_df.to_csv('results/ft-transformers-trainning_val_history_fold_'+fold+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc00654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results - Loss: 0.0058, AUC: 0.9483\n",
      "\u001b[1m4371/4371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 31ms/step\n",
      "\n",
      "Results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#7 - Model Evaluate\n",
    "\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Results - Loss: {results[0]:.4f}, AUC: {results[1]:.4f}\")\n",
    "\n",
    "#probabilities\n",
    "y_pred_probs = model.predict(X_test).flatten()\n",
    "\n",
    "# open auxiliar\n",
    "info_test = pd.read_csv('results/ft-transformers-info_test-fold' + fold + '.csv')[['harpnum', 'T_REC', 'Letra_Class']]\n",
    "info_test = info_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "metrics_list = []\n",
    "positivos_info_total = []\n",
    "probs_com_classe_real = []\n",
    "\n",
    "# --- CSV 3: harpnum + real class + probability + t_rec + Letra_Class ---\n",
    "for idx, (true_class, prob) in enumerate(zip(y_test, y_pred_probs)):\n",
    "    harpnum = info_test.loc[idx, 'harpnum']\n",
    "    t_rec = info_test.loc[idx, 'T_REC']\n",
    "    letra_class = info_test.loc[idx, 'Letra_Class']\n",
    "    probs_com_classe_real.append({\n",
    "        'harpnum': harpnum,\n",
    "        'classe_real': int(true_class),\n",
    "        'probabilidade_modelo': round(float(prob), 6),\n",
    "        'T_REC': t_rec,\n",
    "        'Letra_Class': letra_class\n",
    "    })\n",
    "\n",
    "# --- Loop of thresholds  ---\n",
    "thresholds = np.arange(0.10, 1.00, 0.01)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Métricas\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    computed_loss = bce(y_test, y_pred_probs).numpy()\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    tss = sensitivity + specificity - 1\n",
    "    total = tp + tn + fp + fn\n",
    "    pe = ((tp + fn)*(tp + fp) + (tn + fn)*(tn + fp)) / (total**2)\n",
    "    hss = (accuracy - pe) / (1 - pe) if (1 - pe) != 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "    metrics_list.append({\n",
    "        'threshold': round(threshold, 2),\n",
    "        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
    "        'loss': computed_loss,\n",
    "        'auc': auc_score,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'mcc': mcc,\n",
    "        'tss': tss,\n",
    "        'hss': hss,\n",
    "        'true_positive_rate': sensitivity,\n",
    "        'true_negative_rate': specificity,\n",
    "        'false_positive_rate': fpr,\n",
    "        'false_negative_rate': fnr\n",
    "    })\n",
    "\n",
    "    # --- CSV 2: harpnums positivos únicos + probabilidade + Letra_Class ---\n",
    "    indices_positivos = np.where(y_pred == 1)[0]\n",
    "    for idx in indices_positivos:\n",
    "        prob = y_pred_probs[idx]\n",
    "        harpnum = info_test.loc[idx, 'harpnum']\n",
    "        letra_class = info_test.loc[idx, 'Letra_Class']\n",
    "        positivos_info_total.append({\n",
    "            'threshold': round(threshold, 2),\n",
    "            'harpnum': harpnum,\n",
    "            'probabilidade': round(float(prob), 6),\n",
    "            'Letra_Class': letra_class\n",
    "        })\n",
    "\n",
    "# --- Save files ---\n",
    "# CSV 1: metrics\n",
    "pd.DataFrame(metrics_list).to_csv('results/ft-transformers-metrics-fold' + fold + '.csv', index=False)\n",
    "\n",
    "# CSV 2: unique positives with probability\n",
    "df_positivos = pd.DataFrame(positivos_info_total).drop_duplicates()\n",
    "df_positivos.to_csv('results/ft-transformers-harpnums_all_thresholds_fold' + fold + '.csv', index=False)\n",
    "\n",
    "# CSV 3: all samples with real class and probability\n",
    "pd.DataFrame(probs_com_classe_real).to_csv('results/ft-transformers-real_class_prob_harpnum_' + fold + '.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 (py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
