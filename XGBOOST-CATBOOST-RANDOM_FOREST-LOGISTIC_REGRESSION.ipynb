{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f3b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, QuantileTransformer, PowerTransformer, Normalizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, accuracy_score,\n",
    "    roc_auc_score, f1_score, log_loss, brier_score_loss, average_precision_score,  \n",
    "    balanced_accuracy_score, matthews_corrcoef, classification_report\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    log_loss,                # substitui BinaryCrossentropy do TF\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f47903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - LOADING DATA\n",
    "separator = ','\n",
    "fold_name = \"4\" # set the fold number for the files\n",
    "\n",
    "url_train = \"data/6h/train_df_kfold_\"+fold_name+\".csv\"\n",
    "url_val = \"data/6h/val_df_kfold_\"+fold_name+\".csv\"\n",
    "url_test = \"data/6h/test_df_kfold_\"+fold_name+\".csv\"\n",
    "\n",
    "\n",
    "# CHOOSE COLUMNS TO DELETE\n",
    "\n",
    "qtde_attributes = 18 #18 10\n",
    "\n",
    "#18 attributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class', 'T_REC', 'harpnum']\n",
    "\n",
    "#10 Bobra'attributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class','T_REC', 'harpnum', 'MEANGAM','MEANGBH','MEANGBT','MEANGBZ', 'MEANJZD','MEANJZH','MEANALP','MEANSHR']\n",
    "\n",
    "#10 SHAP'atrributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class', 'T_REC', 'harpnum' ,'ABSNJZH','MEANGAM','MEANJZD','MEANJZH','SAVNCPP','TOTPOT','TOTUSJH','TOTUSJZ']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NORMALIZATION\n",
    "scaler_name = 'StandardScaler'  #'StandardScaler', 'RobustScaler', 'MinMaxScaler', 'NormalizerL1', 'PowerTransformer', 'QuantileTransformer'\n",
    "\n",
    "\n",
    "# BALANCING\n",
    "balanceamento = 'smote'  #'smote', 'oversampling', 'undersampling', 'class_weight', ou 'none'\n",
    "\n",
    "# MODEL \n",
    "model_name  = 'random_forest' #xgboost, catboost, random_forest, logistic_regression\n",
    "\n",
    "\n",
    "epoch = 100\n",
    "batch = 64\n",
    "\n",
    "optimizer=keras.optimizers.Adam(learning_rate=1e-4) #1e4\n",
    "focal_gamma = 2\n",
    "focal_alpha = 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e13d4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juliana Sabino\\AppData\\Local\\Temp\\ipykernel_58516\\558553179.py:3: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(url_train, sep=separator)\n",
      "C:\\Users\\Juliana Sabino\\AppData\\Local\\Temp\\ipykernel_58516\\558553179.py:4: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(url_val, sep=separator)\n",
      "C:\\Users\\Juliana Sabino\\AppData\\Local\\Temp\\ipykernel_58516\\558553179.py:5: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(url_test, sep=separator)\n"
     ]
    }
   ],
   "source": [
    "# 3 - READ AND PREPARE DATA\n",
    "\n",
    "train_df = pd.read_csv(url_train, sep=separator)\n",
    "val_df = pd.read_csv(url_val, sep=separator)\n",
    "test_df = pd.read_csv(url_test, sep=separator)\n",
    "\n",
    "\n",
    "# Convert datetime\n",
    "date1_ta = pd.to_datetime(train_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_ta = pd.to_datetime(train_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "date1_va = pd.to_datetime(val_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_va = pd.to_datetime(val_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "date1_te = pd.to_datetime(test_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_te = pd.to_datetime(test_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "train_df['T_REC'] = date1_ta.fillna(date2_ta)\n",
    "val_df['T_REC'] = date1_va.fillna(date2_va)\n",
    "test_df['T_REC'] = date1_te.fillna(date2_te)\n",
    "\n",
    "# Remove timezone to avoid date shifts\n",
    "train_df['T_REC'] = train_df['T_REC'].dt.tz_localize(None)\n",
    "val_df['T_REC'] = val_df['T_REC'].dt.tz_localize(None)\n",
    "test_df['T_REC'] = test_df['T_REC'].dt.tz_localize(None)\n",
    "\n",
    "# order date\n",
    "train_df = train_df.sort_values(by='T_REC')\n",
    "val_df = val_df.sort_values(by='T_REC')\n",
    "test_df = test_df.sort_values(by='T_REC')\n",
    "\n",
    "\n",
    "#save test extra columns\n",
    "harpnum_test = test_df['harpnum'].values\n",
    "t_rec_test = test_df['T_REC'].values\n",
    "letra_class_test = test_df['Letra_Class'].values\n",
    "\n",
    "info_test = pd.DataFrame({\n",
    "    'harpnum': harpnum_test,\n",
    "    'T_REC': t_rec_test,\n",
    "    'Letra_Class': letra_class_test\n",
    "})\n",
    "info_test.to_csv('results/'+model_name+'-info_test-fold'+fold_name+'.csv', index=False)\n",
    "\n",
    "\n",
    "#delete columns\n",
    "for lcd in list_col_delete:\n",
    "    train_df.pop(lcd)\n",
    "    val_df.pop(lcd)\n",
    "    test_df.pop(lcd)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df20f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USFLUX    float64\n",
      "TOTPOT    float64\n",
      "dtype: object\n",
      "USFLUX    float64\n",
      "TOTPOT    float64\n",
      "dtype: object\n",
      "USFLUX    float64\n",
      "TOTPOT    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 4 - Prepare Data\n",
    "\n",
    "# 1. Separar X e y\n",
    "X_train = train_df.drop(columns='Class')\n",
    "y_train = train_df['Class']\n",
    "\n",
    "X_val = val_df.drop(columns='Class')\n",
    "y_val = val_df['Class']\n",
    "\n",
    "X_test = test_df.drop(columns='Class')\n",
    "y_test = test_df['Class']\n",
    "\n",
    "'''\n",
    "letra_class_col = X_test['Letra_Class'].copy()\n",
    "harpnum_col =  X_test['harpnum'].copy()\n",
    "T_REC_col =  X_test['T_REC'].copy()\n",
    "\n",
    "X_test_model = X_test.drop(columns=['Letra_Class', 'harpnum', 'T_REC'])\n",
    "X_train_model = X_train.drop(columns=['Letra_Class', 'harpnum', 'T_REC'])\n",
    "X_val_model = X_val.drop(columns=['Letra_Class', 'harpnum', 'T_REC'])\n",
    "'''\n",
    "\n",
    "X_test_model = X_test\n",
    "X_train_model = X_train\n",
    "X_val_model = X_val\n",
    "\n",
    "\n",
    "\n",
    "# 5 - Normalization\n",
    "\n",
    "scaler_name = \"StandardScaler\"\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_robust = RobustScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_l1 = Normalizer(norm='l1')\n",
    "transformer_yeo = PowerTransformer(method='yeo-johnson')\n",
    "transformer_quantile = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Choose the scaler based on the scaler_name variable\n",
    "if scaler_name == 'StandardScaler':\n",
    "    scaler = scaler_standard\n",
    "elif scaler_name == 'RobustScaler':\n",
    "    scaler = scaler_robust\n",
    "elif scaler_name == 'MinMaxScaler':\n",
    "    scaler = scaler_minmax\n",
    "elif scaler_name == 'NormalizerL1':\n",
    "    scaler = scaler_l1\n",
    "elif scaler_name == 'PowerTransformer':\n",
    "    scaler = transformer_yeo\n",
    "elif scaler_name == 'QuantileTransformer':\n",
    "    scaler = transformer_quantile\n",
    "else:\n",
    "    raise ValueError(f\"Scaler '{scaler_name}' not recognized. Please choose a valid one.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_model)\n",
    "X_test_scaled = scaler.transform(X_test_model)\n",
    "\n",
    "\n",
    "# 6 -  Balancing\n",
    "\n",
    "if balanceamento == 'smote':\n",
    "    smote = SMOTE(sampling_strategy=0.6, k_neighbors=3, random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "    class_weights = None  # not necessary in this case\n",
    "\n",
    "elif balanceamento == 'oversampling':\n",
    "    oversample = RandomOverSampler(sampling_strategy=0.6, random_state=42)\n",
    "    X_train_res, y_train_res = oversample.fit_resample(X_train, y_train)\n",
    "    class_weights = None\n",
    "\n",
    "elif balanceamento == 'undersampling':\n",
    "    undersample = RandomUnderSampler(sampling_strategy=0.6, random_state=42)\n",
    "    X_train_res, y_train_res = undersample.fit_resample(X_train, y_train)\n",
    "    class_weights = None\n",
    "\n",
    "elif balanceamento == 'class_weight':\n",
    "    # Does not change X_train/y_train, just calculates the weights\n",
    "    X_train_res, y_train_res = X_train, y_train\n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights = dict(zip(classes, weights))\n",
    "\n",
    "elif balanceamento == 'none':\n",
    "    X_train_res, y_train_res = X_train, y_train\n",
    "    class_weights = None\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid balancing method. Choose from: 'smote', 'oversample', 'undersample', 'class_weight', 'none'.\")\n",
    "\n",
    "# 7 - change types\n",
    "for col in ['USFLUX', 'TOTPOT']:\n",
    "    X_train_res[col] = pd.to_numeric(X_train_res[col], errors='coerce')\n",
    "    X_val[col] = pd.to_numeric(X_train_res[col], errors='coerce')\n",
    "    X_test[col] = pd.to_numeric(X_train_res[col], errors='coerce')\n",
    "    \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_res[['USFLUX', 'TOTPOT']] = imputer.fit_transform(X_train_res[['USFLUX', 'TOTPOT']])\n",
    "X_val[['USFLUX', 'TOTPOT']] = imputer.fit_transform(X_val[['USFLUX', 'TOTPOT']])\n",
    "X_test[['USFLUX', 'TOTPOT']] = imputer.fit_transform(X_test[['USFLUX', 'TOTPOT']])\n",
    "\n",
    "\n",
    "\n",
    "print(X_train_res.dtypes.loc[['USFLUX', 'TOTPOT']])\n",
    "print(X_val.dtypes.loc[['USFLUX', 'TOTPOT']])\n",
    "print(X_test.dtypes.loc[['USFLUX', 'TOTPOT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db66837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "if model_name == 'xgboost':\n",
    "\n",
    "    # 8 - XGBoost\n",
    "    neg = (y_train == 0).sum()\n",
    "    pos = (y_train == 1).sum()\n",
    "    scale_pos_weight = neg / pos\n",
    "\n",
    "\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        # — núcleo do boosting —\n",
    "        n_estimators       = 50,     # muitas árvores + learning_rate baixo\n",
    "        learning_rate     = 0.02, \n",
    "        max_depth         = 8,\n",
    "        min_child_weight  = 1,         # controla complexidade dos nós\n",
    "        subsample         = 0.8,       # bagging interno\n",
    "        colsample_bytree  = 0.8,       # aleatoriedade em colunas\n",
    "        gamma             = 0.10,      # exige ganho mínimo p/ dividir nó\n",
    "\n",
    "        # — regularização —\n",
    "        reg_lambda        = 1.0,       # L2\n",
    "        reg_alpha         = 0.10,      # L1\n",
    "\n",
    "        # — dados desbalanceados —\n",
    "        scale_pos_weight  = scale_pos_weight,  # ≈ 190-200 no seu conjunto\n",
    "\n",
    "        # — miscelânea —\n",
    "        objective         = \"binary:logistic\",\n",
    "        eval_metric       = [\"auc\", \"aucpr\", \"logloss\"],   # AUC-PR é mais informativa que logloss aqui\n",
    "        tree_method       = \"hist\",    # rápido em CPU\n",
    "        random_state      = 42,\n",
    "        n_jobs            = -1,\n",
    "        use_label_encoder = False\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) treino com early stopping\n",
    "    # ----------------------------\n",
    "    model.fit(\n",
    "        X_train_res, y_train_res,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        #eval_metric=\"aucpr\",\n",
    "        #early_stopping_rounds=50,\n",
    "        verbose=100          # imprime a cada 100 iterações\n",
    "    )\n",
    "elif model_name == \"catboost\":\n",
    "    # 8 - Catboost\n",
    "    neg = (y_train == 0).sum()\n",
    "    pos = (y_train == 1).sum()\n",
    "    scale_pos_weight = neg / pos\n",
    "\n",
    "\n",
    "    cat_features = [] #because all fields are numeric\n",
    "\n",
    "    train_pool = Pool(\n",
    "        data=X_train_res,\n",
    "        label=y_train_res,\n",
    "        cat_features=cat_features\n",
    "    )\n",
    "\n",
    "    val_pool = Pool(\n",
    "        data=X_val,\n",
    "        label=y_val,\n",
    "        cat_features=cat_features\n",
    "    )\n",
    "\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        # — núcleo do boosting —\n",
    "        iterations        = 150,   #150       # \"n_estimators\" em XGBoost\n",
    "        learning_rate     = 0.09, #0.02\n",
    "        depth             = 7,            # \"max_depth\" 8\n",
    "        min_data_in_leaf  = 2,            # \"min_child_weight\" ~, usa min_data_in_leaf\n",
    "        subsample         = 0.8,          # idem\n",
    "        rsm               = 0.8,          # \"colsample_bytree\"\n",
    "        random_strength   = 0.10,         # ~ gamma (controle de splits aleatórios) 0.1\n",
    "\n",
    "        # — regularização —\n",
    "        l2_leaf_reg       = 1.0,          # reg_lambda\n",
    "        bagging_temperature = 0.1,        # ajuda contra overfitting (semelhante a reg_alpha)\n",
    "\n",
    "        # — dados desbalanceados —\n",
    "        class_weights     = None,  # ou None se preferir sem\n",
    "\n",
    "        # — miscelânea —\n",
    "        loss_function     = 'Logloss',      # otimizado internamente; métricas separadas\n",
    "        eval_metric       = \"PRAUC\",                 # **uma só**\n",
    "        custom_metric     = [\"AUC\", \"Logloss\"],      # recebe “AUC”, “AUC:PR”, “Logloss” etc.\n",
    "        random_seed       = 42,\n",
    "        verbose           = 100,            # imprime a cada 100 iterações\n",
    "        task_type         = 'CPU',          # mude para 'GPU' se tiver suporte\n",
    "        # devices='0'     # se usar GPU, especifique o índice da sua GTX 1650\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 5) treino com early stopping (CatBoost = 'od' / overfitting detector)\n",
    "    # -------------------------------------------------\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        early_stopping_rounds=50,      # igual ao XGBoost\n",
    "        use_best_model=True\n",
    "    )\n",
    "elif model_name == \"random_forest\":\n",
    "    \n",
    "    numeric_features = X_train.columns  # todos numéricos no seu caso\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(with_mean=False), numeric_features),  # evita zeros após SMOTE\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "    \n",
    "    rf_clf = RandomForestClassifier(\n",
    "    n_estimators=600,            # número de árvores\n",
    "    max_depth=50,             # deixe crescer, mas…\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',        # raiz do nº de features por split\n",
    "    class_weight='balanced',    # compensa a minoria (~5 %)\n",
    "    n_jobs=-1,                  # usa todos os núcleos\n",
    "    random_state=42,\n",
    "    oob_score=True              # validação out‑of‑bag (opcional)\n",
    ")\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        ('prep', preprocess),\n",
    "        ('clf', rf_clf)\n",
    "    ])\n",
    "\n",
    "    # Treino\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "        \n",
    "elif model_name == \"logistic_regression\":\n",
    "\n",
    "    model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),             # padroniza média 0, desvio‑padrão 1\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"l2\",                         # regularização padrão\n",
    "        C=1.0,                                # força da regularização (pode ajustar)\n",
    "        solver=\"lbfgs\",                       # bom para datasets médios/grandes\n",
    "        max_iter=1000,                        # mais iterações p/ garantir convergência\n",
    "        class_weight=\"balanced\",              # compensa a minoria\n",
    "        n_jobs=-1,                            # usa todos os núcleos\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results - LogLoss: 0.0516, AUC: 0.9298\n",
      "threshold: 0.5\n",
      "TSS: 0.04007045007316212\n",
      "HSS: 0.0755182751585384\n",
      "TP: 45  TN: 138718  FP: 15  FN: 1075\n",
      "\n",
      "Resultados salvos com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# 9 - Model Evaluate\n",
    "\n",
    "# ── (2) test evaluate ─────────────────────────────────\n",
    "y_pred_probs = model.predict_proba(X_test)[:, 1]      \n",
    "test_loss    = log_loss(y_test, y_pred_probs)        \n",
    "test_auc     = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "print(f\"Test Results - LogLoss: {test_loss:.4f}, AUC: {test_auc:.4f}\")\n",
    "\n",
    "# ── (3) informações auxiliares do fold (inalterado) ───────────────────────────\n",
    "info_test = pd.read_csv(f\"results/\"+model_name+\"-info_test-fold\"+fold_name+\".csv\")[\n",
    "    ['harpnum', 'T_REC', 'Letra_Class']\n",
    "].reset_index(drop=True)\n",
    "\n",
    "metrics_list = []\n",
    "positivos_info_total = []\n",
    "probs_com_classe_real = []\n",
    "\n",
    "# ── (4) CSV-3 – real class + %  ───────────────────────────────────\n",
    "for idx, (true_class, prob) in enumerate(zip(y_test, y_pred_probs)):\n",
    "    harpnum      = info_test.loc[idx, 'harpnum']\n",
    "    t_rec        = info_test.loc[idx, 'T_REC']\n",
    "    letra_class  = info_test.loc[idx, 'Letra_Class']\n",
    "    probs_com_classe_real.append({\n",
    "        'harpnum': harpnum,\n",
    "        'classe_real': int(true_class),\n",
    "        'probabilidade_modelo': round(float(prob), 6),\n",
    "        'T_REC': t_rec,\n",
    "        'Letra_Class': letra_class\n",
    "    })\n",
    "\n",
    "# ── (5)  loop % ────\n",
    "thresholds = np.arange(0.10, 1.00, 0.01)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    # metrics\n",
    "    computed_loss  = log_loss(y_test, y_pred_probs)        # ← agora com sklearn\n",
    "    precision      = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall         = recall_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy       = accuracy_score(y_test, y_pred)\n",
    "    auc_score      = roc_auc_score(y_test, y_pred_probs)\n",
    "    f1             = f1_score(y_test, y_pred, zero_division=0)\n",
    "    balanced_acc   = balanced_accuracy_score(y_test, y_pred)\n",
    "    mcc            = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    # metrics\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    tss = sensitivity + specificity - 1\n",
    "    total = tp + tn + fp + fn\n",
    "    pe = ((tp + fn)*(tp + fp) + (tn + fn)*(tn + fp)) / (total**2)\n",
    "    hss = (accuracy - pe) / (1 - pe) if (1 - pe) != 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "    if round(threshold, 2) == 0.5:\n",
    "        print(\"threshold:\", round(threshold, 2))\n",
    "        print(\"TSS:\", tss)\n",
    "        print(\"HSS:\", hss)\n",
    "        print(\"TP:\", tp, \" TN:\", tn, \" FP:\", fp, \" FN:\", fn)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'threshold': round(threshold, 2),\n",
    "        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
    "        'loss': computed_loss,\n",
    "        'auc': auc_score,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'mcc': mcc,\n",
    "        'tss': tss,\n",
    "        'hss': hss,\n",
    "        'true_positive_rate': sensitivity,\n",
    "        'true_negative_rate': specificity,\n",
    "        'false_positive_rate': fpr,\n",
    "        'false_negative_rate': fnr\n",
    "    })\n",
    "\n",
    "    # CSV-2 – harpnums \n",
    "    for idx in np.where(y_pred == 1)[0]:\n",
    "        harpnum = info_test.loc[idx, 'harpnum']\n",
    "        letra_class = info_test.loc[idx, 'Letra_Class']\n",
    "        positivos_info_total.append({\n",
    "            'threshold': round(threshold, 2),\n",
    "            'harpnum': harpnum,\n",
    "            'probabilidade': round(float(y_pred_probs[idx]), 6),\n",
    "            'Letra_Class': letra_class\n",
    "        })\n",
    "\n",
    "# ── (6) \n",
    "pd.DataFrame(metrics_list).to_csv(f\"results/\"+model_name+\"-metrics-fold\"+fold_name+\".csv\", index=False)\n",
    "\n",
    "df_positivos = pd.DataFrame(positivos_info_total).drop_duplicates()\n",
    "df_positivos.to_csv(f\"results/\"+model_name+\"-harpnums_all_thresholds_fold\"+fold_name+\".csv\", index=False)\n",
    "\n",
    "pd.DataFrame(probs_com_classe_real).to_csv(\n",
    "    f\"results/\"+model_name+\"-real_class_prob_harpnum_\"+fold_name+\".csv\", index=False\n",
    ")\n",
    "\n",
    "print(\"\\nResults saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
