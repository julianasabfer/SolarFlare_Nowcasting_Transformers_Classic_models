{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52f3b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, QuantileTransformer, PowerTransformer, Normalizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, accuracy_score,\n",
    "    roc_auc_score, f1_score, log_loss, brier_score_loss, average_precision_score,  \n",
    "    balanced_accuracy_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97f47903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - LOADING DATA\n",
    "\n",
    "separator = ','\n",
    "fold = \"4\" # set the fold number for the files\n",
    "\n",
    "url_train = \"data/6h/train_df_kfold_\"+fold+\".csv\"\n",
    "url_val = \"data/6h/val_df_kfold_\"+fold+\".csv\"\n",
    "url_test = \"data/6h/test_df_kfold_\"+fold+\".csv\"\n",
    "\n",
    "\n",
    "# CHOOSE COLUMNS TO DELETE\n",
    "\n",
    "qtde_attributes = 18 #18 10\n",
    "\n",
    "\n",
    "#18 attributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class']\n",
    "\n",
    "#10 Bobra'attributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class', 'MEANGAM','MEANGBH','MEANGBT','MEANGBZ', 'MEANJZD','MEANJZH','MEANALP','MEANSHR']\n",
    "\n",
    "#10 SHAP'atrributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class', 'ABSNJZH','MEANGAM','MEANJZD','MEANJZH','SAVNCPP','TOTPOT','TOTUSJH','TOTUSJZ']\n",
    "\n",
    "\n",
    "# NORMALIZATION\n",
    "scaler_name = 'StandardScaler'  #'StandardScaler', 'RobustScaler', 'MinMaxScaler', 'NormalizerL1', 'PowerTransformer', 'QuantileTransformer'\n",
    "\n",
    "\n",
    "# BALANCING\n",
    "balanceamento = 'smote'  #'smote', 'oversampling', 'undersampling', 'class_weight', ou 'none'\n",
    "\n",
    "# Transformer  model configuration\n",
    "head_size=192\n",
    "num_heads=12\n",
    "ff_dim = 256\n",
    "num_transformer_blocks= 6\n",
    "mlp_units = [128, 64, 32]\n",
    "dropout = 0.2\n",
    "mlp_dropout = 0.2\n",
    "\n",
    "epoch = 1\n",
    "batch = 512\n",
    "\n",
    "optimizer=keras.optimizers.Adam(learning_rate=1e-4) #1e4\n",
    "focal_gamma = 2\n",
    "focal_alpha = 0.25\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e13d4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_25492\\2373590900.py:3: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(url_train, sep=separator)\n",
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_25492\\2373590900.py:4: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(url_val, sep=separator)\n",
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_25492\\2373590900.py:5: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(url_test, sep=separator)\n"
     ]
    }
   ],
   "source": [
    "# 3 - READ AND PREPARE DATA\n",
    "\n",
    "train_df = pd.read_csv(url_train, sep=separator)\n",
    "val_df = pd.read_csv(url_val, sep=separator)\n",
    "test_df = pd.read_csv(url_test, sep=separator)\n",
    "\n",
    "\n",
    "# Convert datetime\n",
    "date1_ta = pd.to_datetime(train_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_ta = pd.to_datetime(train_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "date1_va = pd.to_datetime(val_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_va = pd.to_datetime(val_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "date1_te = pd.to_datetime(test_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_te = pd.to_datetime(test_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "train_df['T_REC'] = date1_ta.fillna(date2_ta)\n",
    "val_df['T_REC'] = date1_va.fillna(date2_va)\n",
    "test_df['T_REC'] = date1_te.fillna(date2_te)\n",
    "\n",
    "# Remove timezone to avoid date shifts\n",
    "train_df['T_REC'] = train_df['T_REC'].dt.tz_localize(None)\n",
    "val_df['T_REC'] = val_df['T_REC'].dt.tz_localize(None)\n",
    "test_df['T_REC'] = test_df['T_REC'].dt.tz_localize(None)\n",
    "\n",
    "# order date\n",
    "train_df = train_df.sort_values(by='T_REC')\n",
    "val_df = val_df.sort_values(by='T_REC')\n",
    "test_df = test_df.sort_values(by='T_REC')\n",
    "\n",
    "\n",
    "#save test extra columns\n",
    "harpnum_test = test_df['harpnum'].values\n",
    "t_rec_test = test_df['T_REC'].values\n",
    "letra_class_test = test_df['Letra_Class'].values\n",
    "\n",
    "info_test = pd.DataFrame({\n",
    "    'harpnum': harpnum_test,\n",
    "    'T_REC': t_rec_test,\n",
    "    'Letra_Class': letra_class_test\n",
    "})\n",
    "info_test.to_csv('results/window-transformers-info_test-fold'+fold+'.csv', index=False)\n",
    "\n",
    "\n",
    "#delete columns\n",
    "for lcd in list_col_delete:\n",
    "    train_df.pop(lcd)\n",
    "    val_df.pop(lcd)\n",
    "    test_df.pop(lcd)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97a0bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (321616, 36, 10)\n",
      "y_train shape: (321616,)\n",
      "x_val shape: (79260, 36, 10)\n",
      "x_test shape: (102572, 36, 10)\n",
      "New x_train shape after SMOTE: (510364, 36, 10)\n",
      "New y_train shape after SMOTE: (510364,)\n",
      "\u001b[1m997/997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8293s\u001b[0m 8s/step - auc: 0.9529 - fn: 55899.0000 - fp: 16640.0000 - loss: 0.0297 - precision: 0.8906 - recall: 0.7079 - tn: 302338.0000 - tp: 135487.0000 - val_auc: 0.9541 - val_fn: 207.0000 - val_fp: 4068.0000 - val_loss: 0.0210 - val_precision: 0.1114 - val_recall: 0.7113 - val_tn: 74475.0000 - val_tp: 510.0000 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#Create sequences\n",
    "\n",
    "SEQUENCE_SIZE = 36  # Number of previous steps\n",
    "FEATURES = [col for col in train_df.columns if col not in ['Class', 'T_REC', 'harpnum']]\n",
    "\n",
    "def create_sequences(df, sequence_size, features):\n",
    "    \"\"\"Gera sequências respeitando o harpnum.\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for harpnum, group in df.groupby('harpnum'):\n",
    "        group = group.sort_values('T_REC')  # Ordena por tempo\n",
    "        data = group[features].values\n",
    "        labels = group['Class'].values\n",
    "        \n",
    "        for i in range(len(group) - sequence_size):\n",
    "            window = data[i:i+sequence_size]\n",
    "            label = labels[i+sequence_size]  # Prever o evento seguinte\n",
    "            X.append(window)\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "x_train, y_train = create_sequences(train_df, SEQUENCE_SIZE, FEATURES)\n",
    "x_val, y_val = create_sequences(val_df, SEQUENCE_SIZE, FEATURES)\n",
    "x_test, y_test = create_sequences(test_df, SEQUENCE_SIZE, FEATURES)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_val shape: {x_val.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "#Normalization\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_robust = RobustScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_l1 = Normalizer(norm='l1')\n",
    "transformer_yeo = PowerTransformer(method='yeo-johnson')\n",
    "transformer_quantile = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Choose the scaler based on the scaler_name variable\n",
    "if scaler_name == 'StandardScaler':\n",
    "    scaler = scaler_standard\n",
    "elif scaler_name == 'RobustScaler':\n",
    "    scaler = scaler_robust\n",
    "elif scaler_name == 'MinMaxScaler':\n",
    "    scaler = scaler_minmax\n",
    "elif scaler_name == 'NormalizerL1':\n",
    "    scaler = scaler_l1\n",
    "elif scaler_name == 'PowerTransformer':\n",
    "    scaler = transformer_yeo\n",
    "elif scaler_name == 'QuantileTransformer':\n",
    "    scaler = transformer_quantile\n",
    "else:\n",
    "    raise ValueError(f\"Scaler '{scaler_name}' não reconhecido. Escolha um válido.\")\n",
    "\n",
    "\n",
    "\n",
    "# Colapse for  2D\n",
    "x_train_2d = x_train.reshape(-1, x_train.shape[-1])\n",
    "x_val_2d = x_val.reshape(-1, x_val.shape[-1])\n",
    "x_test_2d = x_test.reshape(-1, x_test.shape[-1])\n",
    "\n",
    "# Fit only train\n",
    "scaler.fit(x_train_2d)\n",
    "\n",
    "# Apply transformationcha\n",
    "x_train = scaler.transform(x_train_2d).reshape(x_train.shape)\n",
    "x_val = scaler.transform(x_val_2d).reshape(x_val.shape)\n",
    "x_test = scaler.transform(x_test_2d).reshape(x_test.shape)\n",
    "\n",
    "# Reshape\n",
    "n_samples, seq_len, n_features = x_train.shape\n",
    "x_train_flat = x_train.reshape((n_samples, seq_len * n_features))\n",
    "\n",
    "x_train_flat = x_train.reshape((n_samples, seq_len * n_features))\n",
    "\n",
    "#Balancing\n",
    "if balanceamento == 'smote':\n",
    "    smote = SMOTE(sampling_strategy=0.6, k_neighbors=3, random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(x_train_flat, y_train)\n",
    "    class_weights = None  # not necessary in this case\n",
    "\n",
    "elif balanceamento == 'oversampling':\n",
    "    oversample = RandomOverSampler(sampling_strategy=0.6, random_state=42)\n",
    "    X_train_res, y_train_res = oversample.fit_resample(x_train, y_train)\n",
    "    class_weights = None\n",
    "\n",
    "elif balanceamento == 'undersampling':\n",
    "    undersample = RandomUnderSampler(sampling_strategy=0.6, random_state=42)\n",
    "    X_train_res, y_train_res = undersample.fit_resample(x_train, y_train)\n",
    "    class_weights = None\n",
    "\n",
    "elif balanceamento == 'class_weight':\n",
    "    # Does not change X_train/y_train, just calculates the weights\n",
    "    X_train_res, y_train_res = x_train, y_train\n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights = dict(zip(classes, weights))\n",
    "\n",
    "elif balanceamento == 'none':\n",
    "    X_train_res, y_train_res = x_train, y_train\n",
    "    class_weights = None\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid balancing method. Choose from: 'smote', 'oversample', 'undersample', 'class_weight', 'none'.\")\n",
    "\n",
    "\n",
    "\n",
    "# Return to sequential format\n",
    "x_train = X_train_res.reshape((-1, seq_len, n_features))\n",
    "y_train = y_train_res\n",
    "\n",
    "print(f\"New x_train shape after SMOTE: {x_train.shape}\")\n",
    "print(f\"New y_train shape after SMOTE: {y_train.shape}\")\n",
    "\n",
    "\n",
    "# Transformers' Model\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "    \n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    \n",
    "    return x + res\n",
    "\n",
    "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "\n",
    "# --- Focal loss ---\n",
    "def binary_focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        modulating_factor = tf.pow((1 - p_t), gamma)\n",
    "        return tf.reduce_mean(alpha_factor * modulating_factor * bce)\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=head_size,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_transformer_blocks=num_transformer_blocks,\n",
    "    mlp_units= mlp_units,\n",
    "    mlp_dropout= mlp_dropout,\n",
    "    dropout= dropout\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=binary_focal_loss(gamma=focal_gamma, alpha=focal_alpha),\n",
    "    metrics=[\n",
    "        AUC(name='auc'),\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "        TruePositives(name='tp'),\n",
    "        TrueNegatives(name='tn'),\n",
    "        FalsePositives(name='fp'),\n",
    "        FalseNegatives(name='fn')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csv_logger = CSVLogger('results/window-transformers-trainning_log_fold_'+fold+'.csv', append=True)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', patience=10, mode='max', restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.keras', monitor='val_auc', mode='max', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, mode='max', verbose=1),\n",
    "    csv_logger\n",
    "]\n",
    "\n",
    "\n",
    "if class_weights == None:\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epoch,\n",
    "        batch_size=batch,\n",
    "        callbacks=callbacks, \n",
    "        verbose=1\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epoch,\n",
    "        batch_size=batch,\n",
    "        callbacks=callbacks,\n",
    "        class_weights = class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4900e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 - Salve training e val metrics\n",
    "\n",
    "\n",
    "# Convert history to DataFrame\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "\n",
    "\n",
    "# Saved to a CSV file\n",
    "history_df.to_csv('results/window-transformers-trainning_val_history_fold_'+fold+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc00654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results - Loss: 0.0178, AUC: 0.9518\n",
      "\u001b[1m3206/3206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 85ms/step\n",
      "\n",
      "Results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#7 - Model Evaluate\n",
    "\n",
    "\n",
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Results - Loss: {results[0]:.4f}, AUC: {results[1]:.4f}\")\n",
    "\n",
    "#probabilities\n",
    "y_pred_probs = model.predict(x_test).flatten()\n",
    "\n",
    "# open auxiliar\n",
    "info_test = pd.read_csv('results/window-transformers-info_test-fold' + fold + '.csv')[['harpnum', 'T_REC', 'Letra_Class']]\n",
    "info_test = info_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "metrics_list = []\n",
    "positivos_info_total = []\n",
    "probs_com_classe_real = []\n",
    "\n",
    "# --- CSV 3: harpnum + real class + probability + t_rec + Letra_Class ---\n",
    "for idx, (true_class, prob) in enumerate(zip(y_test, y_pred_probs)):\n",
    "    harpnum = info_test.loc[idx, 'harpnum']\n",
    "    t_rec = info_test.loc[idx, 'T_REC']\n",
    "    letra_class = info_test.loc[idx, 'Letra_Class']\n",
    "    probs_com_classe_real.append({\n",
    "        'harpnum': harpnum,\n",
    "        'classe_real': int(true_class),\n",
    "        'probabilidade_modelo': round(float(prob), 6),\n",
    "        'T_REC': t_rec,\n",
    "        'Letra_Class': letra_class\n",
    "    })\n",
    "\n",
    "# --- Loop of thresholds  ---\n",
    "thresholds = np.arange(0.10, 1.00, 0.01)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Métricas\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    computed_loss = bce(y_test, y_pred_probs).numpy()\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    tss = sensitivity + specificity - 1\n",
    "    total = tp + tn + fp + fn\n",
    "    pe = ((tp + fn)*(tp + fp) + (tn + fn)*(tn + fp)) / (total**2)\n",
    "    hss = (accuracy - pe) / (1 - pe) if (1 - pe) != 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "    metrics_list.append({\n",
    "        'threshold': round(threshold, 2),\n",
    "        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
    "        'loss': computed_loss,\n",
    "        'auc': auc_score,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'mcc': mcc,\n",
    "        'tss': tss,\n",
    "        'hss': hss,\n",
    "        'true_positive_rate': sensitivity,\n",
    "        'true_negative_rate': specificity,\n",
    "        'false_positive_rate': fpr,\n",
    "        'false_negative_rate': fnr\n",
    "    })\n",
    "\n",
    "    # --- CSV 2: harpnums positivos únicos + probabilidade + Letra_Class ---\n",
    "    indices_positivos = np.where(y_pred == 1)[0]\n",
    "    for idx in indices_positivos:\n",
    "        prob = y_pred_probs[idx]\n",
    "        harpnum = info_test.loc[idx, 'harpnum']\n",
    "        letra_class = info_test.loc[idx, 'Letra_Class']\n",
    "        positivos_info_total.append({\n",
    "            'threshold': round(threshold, 2),\n",
    "            'harpnum': harpnum,\n",
    "            'probabilidade': round(float(prob), 6),\n",
    "            'Letra_Class': letra_class\n",
    "        })\n",
    "\n",
    "# --- Save files ---\n",
    "# CSV 1: metrics\n",
    "pd.DataFrame(metrics_list).to_csv('results/window-transformers-metrics-fold' + fold + '.csv', index=False)\n",
    "\n",
    "# CSV 2: unique positives with probability\n",
    "df_positivos = pd.DataFrame(positivos_info_total).drop_duplicates()\n",
    "df_positivos.to_csv('results/window-transformers-harpnums_all_thresholds_fold' + fold + '.csv', index=False)\n",
    "\n",
    "# CSV 3: all samples with real class and probability\n",
    "pd.DataFrame(probs_com_classe_real).to_csv('results/window-transformers-real_class_prob_harpnum_' + fold + '.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 (py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
