{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f3b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, QuantileTransformer, PowerTransformer, Normalizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, accuracy_score,\n",
    "    roc_auc_score, f1_score, log_loss, brier_score_loss, average_precision_score,  \n",
    "    balanced_accuracy_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f47903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - LOADING DATA\n",
    "\n",
    "separator = ','\n",
    "fold = \"4\" # set the fold number for the files\n",
    "\n",
    "url_train = \"data/6h/train_df_kfold_\"+fold+\".csv\"\n",
    "url_val = \"data/6h/val_df_kfold_\"+fold+\".csv\"\n",
    "url_test = \"data/6h/test_df_kfold_\"+fold+\".csv\"\n",
    "\n",
    "\n",
    "# CHOOSE COLUMNS TO DELETE\n",
    "\n",
    "qtde_attributes = 18 #18 10\n",
    "\n",
    "#18 attributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class', 'T_REC', 'harpnum']\n",
    "\n",
    "#10 Bobra'attributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class','T_REC', 'harpnum', 'MEANGAM','MEANGBH','MEANGBT','MEANGBZ', 'MEANJZD','MEANJZH','MEANALP','MEANSHR']\n",
    "\n",
    "#10 SHAP'atrributes\n",
    "list_col_delete = ['Class_Flare','Letra_Class', 'T_REC', 'harpnum', 'ABSNJZH','MEANGAM','MEANJZD','MEANJZH','SAVNCPP','TOTPOT','TOTUSJH','TOTUSJZ']\n",
    "\n",
    "\n",
    "\n",
    "# NORMALIZATION\n",
    "scaler_name = 'StandardScaler'  #'StandardScaler', 'RobustScaler', 'MinMaxScaler', 'NormalizerL1', 'PowerTransformer', 'QuantileTransformer'\n",
    "\n",
    "\n",
    "# BALANCING\n",
    "balanceamento = 'smote'  #'smote', 'oversampling', 'undersampling', 'class_weight'\n",
    "\n",
    "#Transformer  model configuration\n",
    "head_size = 128\n",
    "num_heads = 8\n",
    "ff_dim = 128\n",
    "num_transformer_blocks = 6\n",
    "mlp_units = [256, 128, 64, 32]\n",
    "dropout = 0.2\n",
    "mlp_dropout = 0.2\n",
    "\n",
    "epoch = 100 \n",
    "batch = 64  \n",
    "optimizer=keras.optimizers.Adam(learning_rate=1e-4) \n",
    "focal_gamma = 2\n",
    "focal_alpha = 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e13d4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_20388\\2051583829.py:3: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(url_train, sep=separator)\n",
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_20388\\2051583829.py:4: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(url_val, sep=separator)\n",
      "C:\\Users\\JulianaSabinoFerreir\\AppData\\Local\\Temp\\ipykernel_20388\\2051583829.py:5: DtypeWarning: Columns (3,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(url_test, sep=separator)\n"
     ]
    }
   ],
   "source": [
    "# 3 - READ AND PREPARE DATA\n",
    "\n",
    "train_df = pd.read_csv(url_train, sep=separator)\n",
    "val_df = pd.read_csv(url_val, sep=separator)\n",
    "test_df = pd.read_csv(url_test, sep=separator)\n",
    "\n",
    "\n",
    "# Convert datetime\n",
    "date1_ta = pd.to_datetime(train_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_ta = pd.to_datetime(train_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "date1_va = pd.to_datetime(val_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_va = pd.to_datetime(val_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "date1_te = pd.to_datetime(test_df['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2_te = pd.to_datetime(test_df['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "train_df['T_REC'] = date1_ta.fillna(date2_ta)\n",
    "val_df['T_REC'] = date1_va.fillna(date2_va)\n",
    "test_df['T_REC'] = date1_te.fillna(date2_te)\n",
    "\n",
    "# Remove timezone to avoid date shifts\n",
    "train_df['T_REC'] = train_df['T_REC'].dt.tz_localize(None)\n",
    "val_df['T_REC'] = val_df['T_REC'].dt.tz_localize(None)\n",
    "test_df['T_REC'] = test_df['T_REC'].dt.tz_localize(None)\n",
    "\n",
    "# order date\n",
    "train_df = train_df.sort_values(by='T_REC')\n",
    "val_df = val_df.sort_values(by='T_REC')\n",
    "test_df = test_df.sort_values(by='T_REC')\n",
    "\n",
    "\n",
    "#save test extra columns\n",
    "harpnum_test = test_df['harpnum'].values\n",
    "t_rec_test = test_df['T_REC'].values\n",
    "letra_class_test = test_df['Letra_Class'].values\n",
    "\n",
    "info_test = pd.DataFrame({\n",
    "    'harpnum': harpnum_test,\n",
    "    'T_REC': t_rec_test,\n",
    "    'Letra_Class': letra_class_test\n",
    "})\n",
    "info_test.to_csv('results/transformers-info_test-fold'+fold+'.csv', index=False)\n",
    "\n",
    "\n",
    "#delete columns\n",
    "for lcd in list_col_delete:\n",
    "    train_df.pop(lcd)\n",
    "    val_df.pop(lcd)\n",
    "    test_df.pop(lcd)\n",
    "    \n",
    "\n",
    "target_col = 'Class'\n",
    "X_train = train_df.drop(columns=[target_col]).values\n",
    "y_train = train_df[target_col].values\n",
    "X_val = val_df.drop(columns=[target_col]).values\n",
    "y_val = val_df[target_col].values\n",
    "X_test = test_df.drop(columns=[target_col]).values\n",
    "y_test = test_df[target_col].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9c79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 - NORMALIZATION AND BALANCING\n",
    "\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_robust = RobustScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_l1 = Normalizer(norm='l1')\n",
    "transformer_yeo = PowerTransformer(method='yeo-johnson')\n",
    "transformer_quantile = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Choose the scaler based on the scaler_name variable\n",
    "if scaler_name == 'StandardScaler':\n",
    "    scaler = scaler_standard\n",
    "elif scaler_name == 'RobustScaler':\n",
    "    scaler = scaler_robust\n",
    "elif scaler_name == 'MinMaxScaler':\n",
    "    scaler = scaler_minmax\n",
    "elif scaler_name == 'NormalizerL1':\n",
    "    scaler = scaler_l1\n",
    "elif scaler_name == 'PowerTransformer':\n",
    "    scaler = transformer_yeo\n",
    "elif scaler_name == 'QuantileTransformer':\n",
    "    scaler = transformer_quantile\n",
    "else:\n",
    "    raise ValueError(f\"Scaler '{scaler_name}' não reconhecido. Escolha um válido.\")\n",
    "\n",
    "# Apply the chosen scaler\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if balanceamento == 'smote':\n",
    "    smote = SMOTE(sampling_strategy=1, k_neighbors=3, random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "    class_weights = None  # not necessary in this case\n",
    "\n",
    "elif balanceamento == 'oversampling':\n",
    "    oversample = RandomOverSampler(sampling_strategy=0.6, random_state=42)\n",
    "    X_train_res, y_train_res = oversample.fit_resample(X_train, y_train)\n",
    "    class_weights = None\n",
    "\n",
    "elif balanceamento == 'undersampling':\n",
    "    undersample = RandomUnderSampler(sampling_strategy=0.6, random_state=42)\n",
    "    X_train_res, y_train_res = undersample.fit_resample(X_train, y_train)\n",
    "    class_weights = None\n",
    "\n",
    "elif balanceamento == 'class_weight':\n",
    "    # Does not change X_train/y_train, just calculates the weights\n",
    "    X_train_res, y_train_res = X_train, y_train\n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights = dict(zip(classes, weights))\n",
    "\n",
    "elif balanceamento == 'none':\n",
    "    X_train_res, y_train_res = X_train, y_train\n",
    "    class_weights = None\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Método de balanceamento inválido. Escolha entre: 'smote', 'oversample', 'undersample', 'class_weight', 'none'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4973f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1722/1722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 262ms/step - auc: 0.8281 - fn: 2923.0000 - fp: 351.0000 - loss: 0.0064 - precision: 0.1269 - recall: 0.0171 - tn: 437483.0000 - tp: 51.0000 - val_auc: 0.9416 - val_fn: 857.0000 - val_fp: 0.0000e+00 - val_loss: 0.0036 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_tn: 107907.0000 - val_tp: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#5- TRANSFORMEFS MODEL\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the transformer encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization (epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = layers. Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "    \n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x =layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    \n",
    "    return x + res\n",
    "\n",
    "\n",
    "# Build the transformer model\n",
    "def build_model_transformers(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0): \n",
    "    #inputs = keras.Input(shape=input_shape)\n",
    "    inputs = keras.Input(shape=(input_shape, 1))\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    \n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    \n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "        \n",
    "    outputs = layers. Dense (1)(x)\n",
    "    return keras. Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        modulating_factor = tf.pow((1 - p_t), gamma)\n",
    "        return tf.reduce_mean(alpha_factor * modulating_factor * bce)\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Create model ---\n",
    "model = build_model_transformers(\n",
    "    input_shape=n_features,\n",
    "    head_size=head_size,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim = ff_dim,\n",
    "    num_transformer_blocks= num_transformer_blocks,\n",
    "    mlp_units = mlp_units,\n",
    "    dropout = dropout, \n",
    "    mlp_dropout = mlp_dropout\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#compile model \n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=binary_focal_loss(gamma=focal_gamma, alpha=focal_alpha),\n",
    "    metrics=[\n",
    "        AUC(name='auc'),\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "        TruePositives(name='tp'),\n",
    "        TrueNegatives(name='tn'),\n",
    "        FalsePositives(name='fp'),\n",
    "        FalseNegatives(name='fn')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csv_logger = CSVLogger('results/transformers-trainning_log_fold_'+fold+'.csv', append=True)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', patience=10, mode='max', restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.keras', monitor='val_auc', mode='max', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, mode='max', verbose=1),\n",
    "    csv_logger\n",
    "]\n",
    "\n",
    "\n",
    "#Fit Model\n",
    "\n",
    "if class_weights == None:\n",
    "    history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epoch,\n",
    "    batch_size=batch,\n",
    "    verbose=1\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epoch,\n",
    "    batch_size=batch,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4900e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 - Salve training e val metrics\n",
    "\n",
    "\n",
    "# Convert history to DataFrame\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "\n",
    "\n",
    "# Saved to a CSV file\n",
    "history_df.to_csv('results/transformers-trainning_val_history_fold_'+fold+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc00654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results - Loss: 0.0034, AUC: 0.9530\n",
      "\u001b[1m4371/4371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 19ms/step\n",
      "\n",
      "Results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#7 - Model Evaluate\n",
    "\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Results - Loss: {results[0]:.4f}, AUC: {results[1]:.4f}\")\n",
    "\n",
    "#probabilities\n",
    "y_pred_probs = model.predict(X_test).flatten()\n",
    "\n",
    "# open auxiliar\n",
    "info_test = pd.read_csv('results/transformers-info_test-fold' + fold + '.csv')[['harpnum', 'T_REC', 'Letra_Class']]\n",
    "info_test = info_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "metrics_list = []\n",
    "positivos_info_total = []\n",
    "probs_com_classe_real = []\n",
    "\n",
    "# --- CSV 3: harpnum + real class + probability + t_rec + Letra_Class ---\n",
    "for idx, (true_class, prob) in enumerate(zip(y_test, y_pred_probs)):\n",
    "    harpnum = info_test.loc[idx, 'harpnum']\n",
    "    t_rec = info_test.loc[idx, 'T_REC']\n",
    "    letra_class = info_test.loc[idx, 'Letra_Class']\n",
    "    probs_com_classe_real.append({\n",
    "        'harpnum': harpnum,\n",
    "        'classe_real': int(true_class),\n",
    "        'probabilidade_modelo': round(float(prob), 6),\n",
    "        'T_REC': t_rec,\n",
    "        'Letra_Class': letra_class\n",
    "    })\n",
    "\n",
    "# --- Loop of thresholds  ---\n",
    "thresholds = np.arange(0.10, 1.00, 0.01)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Métricas\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    computed_loss = bce(y_test, y_pred_probs).numpy()\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    tss = sensitivity + specificity - 1\n",
    "    total = tp + tn + fp + fn\n",
    "    pe = ((tp + fn)*(tp + fp) + (tn + fn)*(tn + fp)) / (total**2)\n",
    "    hss = (accuracy - pe) / (1 - pe) if (1 - pe) != 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "    metrics_list.append({\n",
    "        'threshold': round(threshold, 2),\n",
    "        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
    "        'loss': computed_loss,\n",
    "        'auc': auc_score,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'mcc': mcc,\n",
    "        'tss': tss,\n",
    "        'hss': hss,\n",
    "        'true_positive_rate': sensitivity,\n",
    "        'true_negative_rate': specificity,\n",
    "        'false_positive_rate': fpr,\n",
    "        'false_negative_rate': fnr\n",
    "    })\n",
    "\n",
    "    # --- CSV 2: harpnums positivos únicos + probabilidade + Letra_Class ---\n",
    "    indices_positivos = np.where(y_pred == 1)[0]\n",
    "    for idx in indices_positivos:\n",
    "        prob = y_pred_probs[idx]\n",
    "        harpnum = info_test.loc[idx, 'harpnum']\n",
    "        letra_class = info_test.loc[idx, 'Letra_Class']\n",
    "        positivos_info_total.append({\n",
    "            'threshold': round(threshold, 2),\n",
    "            'harpnum': harpnum,\n",
    "            'probabilidade': round(float(prob), 6),\n",
    "            'Letra_Class': letra_class\n",
    "        })\n",
    "\n",
    "# --- Save files ---\n",
    "# CSV 1: metrics\n",
    "pd.DataFrame(metrics_list).to_csv('results/transformers-metrics-fold' + fold + '.csv', index=False)\n",
    "\n",
    "# CSV 2: unique positives with probability\n",
    "df_positivos = pd.DataFrame(positivos_info_total).drop_duplicates()\n",
    "df_positivos.to_csv('results/transformers-harpnums_all_thresholds_fold' + fold + '.csv', index=False)\n",
    "\n",
    "# CSV 3: all samples with real class and probability\n",
    "pd.DataFrame(probs_com_classe_real).to_csv('results/transformers-real_class_prob_harpnum_' + fold + '.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 (py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
