{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juliana\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (5.1.0)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#1 - libraries\n",
    "import drms\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from sunpy.time import TimeRange\n",
    "import sunkit_instruments.goes_xrs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime, date, time, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#2 - data range collect\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2024-04-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 - data colect from GOES\n",
    "\n",
    "if os.path.isfile('data/goes_data_'+str(start_date)+'_'+str(end_date)+'.npy'):\n",
    "    goes_events = np.load('data/goes_data_'+str(start_date)+'_'+str(end_date)+'.npy', allow_pickle=True, encoding='latin1')      \n",
    "else:\n",
    "    print(\"Download new GOES data\")\n",
    "    time_range = TimeRange(start_date, end_date)\n",
    "    goes_events = sunkit_instruments.goes_xrs.get_goes_event_list(time_range,\"M1\") \n",
    "\n",
    "    np.save('data/goes_data_'+str(start_date)+'_'+str(end_date)+'.npy', goes_events)\n",
    "\n",
    "    lista_goes = []\n",
    "\n",
    "    for ge in goes_events:\n",
    "        item_goes = []\n",
    "        item_goes.append(ge['event_date'])\n",
    "        item_goes.append(ge['start_time'])\n",
    "        item_goes.append(ge['peak_time'])\n",
    "        item_goes.append(ge['end_time'])\n",
    "        item_goes.append(ge['goes_class'])\n",
    "        item_goes.append(ge['goes_location'])\n",
    "        item_goes.append(ge['noaa_active_region'])\n",
    "        lista_goes.append(item_goes)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(lista_goes)\n",
    "    df.to_csv('data/goes_data_'+str(start_date)+'_'+str(end_date)+'.csv')\n",
    "\n",
    "\n",
    "    \n",
    "# map active regions  NOAA e HARPNUMs \n",
    "if os.path.isfile('data/all_harps_with_noaa_ars.txt'):\n",
    "    num_mapper = pd.read_csv('data/all_harps_with_noaa_ars.txt', sep=' ')\n",
    "else:\n",
    "    num_mapper = pd.read_csv('http://jsoc.stanford.edu/doc/data/hmi/harpnum_to_noaa/all_harps_with_noaa_ars.txt',sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 - general functions\n",
    "\n",
    "def from_tai_time(tstr):\n",
    "    \"\"\"\n",
    "    Convert the time string to a dt object\n",
    "    \"\"\"\n",
    "    return dt.datetime.strptime(tstr, \"%Y.%m.%d_%H:%M:%S_TAI\")\n",
    "\n",
    "def to_tai_time(date):\n",
    "    \"\"\"\n",
    "    Convert from a dt object to a time string object\n",
    "    \"\"\"\n",
    "    #original  .strftime\n",
    "    return date.strftime(\"%Y.%m.%d_%H:%M:%S_TAI\")\n",
    "\n",
    "def round_up(time):\n",
    "    \"\"\"\n",
    "    Rounds up the time to the next time divisible by 12 minutes (JSOC data is saved every 12 minutes)\n",
    "    \n",
    "    Args:\n",
    "    time := td object\n",
    "    \"\"\"\n",
    "    #remainder = time.minute % 12\n",
    "    remainder =  (int( str(time)[14:16] )) % 12\n",
    "\n",
    "    if remainder != 0:\n",
    "        time = time + dt.timedelta(minutes=12-remainder)\n",
    "    return time\n",
    "\n",
    "\n",
    "def convert_noaa_to_harpnum(noaa_ar):\n",
    "    \"\"\"\n",
    "    Converts from a NOAA Active Region to a HARPNUM\n",
    "    Returns harpnum if present, else None if there are no matching harpnums\n",
    "    \n",
    "    Args:\n",
    "    \"\"\"\n",
    "    idx = num_mapper[num_mapper['NOAA_ARS'].str.contains(str(int(noaa_ar)))]\n",
    "    return None if idx.empty else idx.HARPNUM.values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 - collect goes-data from JSOC - events M, X\n",
    "\n",
    "\n",
    "for event in goes_events:\n",
    "    harpnum = convert_noaa_to_harpnum(event['noaa_active_region'])\n",
    "    goes_class = event['goes_class']\n",
    "   \n",
    "    \n",
    "    if harpnum == None:\n",
    "        print('there are no matching HARPNUMs for', str(int(event['noaa_active_region'])))\n",
    "        continue\n",
    "\n",
    "    #Subtracts the forecast window from the GOES event date\n",
    "    date_event = event['peak_time'].to_datetime() - dt.timedelta(hours=6)\n",
    " \n",
    "    #connect SHARP dataset\n",
    "    c = drms.Client(server='jsoc', email='julianasabfer@gmail.com', verbose=False, debug=False)\n",
    "    keys, segments = c.query('hmi.sharp_720s['+str(harpnum)+']['+str(date_event)+'] ', key='OBS_VR,QUALITY,DATE,DATE_S,DATE_B,DATE__OBS,DATE-OBS,T_OBS,T_REC,harpnum,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANALP,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR', seg='Br')\n",
    "    df = pd.DataFrame(keys)\n",
    "\n",
    "   \n",
    "    if keys.empty:\n",
    "        print(\"there are no matching data for datetime: \", str(date_event))\n",
    "        continue\n",
    "\n",
    "    \n",
    "    keys[\"Class_Flare\"] = [goes_class]\n",
    "\n",
    "    #Assigns binary class to events\n",
    "    if \"A\" in goes_class or \"B\" in goes_class or \"C\" in goes_class:\n",
    "        keys[\"Class\"] = [0]\n",
    "    elif \"M\" in goes_class or \"X\" in goes_class:\n",
    "        keys[\"Class\"] = [1]\n",
    "\n",
    "    #save results\n",
    "    if(os.path.isfile('data/sharp_mx_'+str(start_date)+'_'+str(end_date)+'.csv')):\n",
    "        keys.to_csv('data/sharp_mx_'+str(start_date)+'_'+str(end_date)+'.csv', mode='a', index= False,header=False)\n",
    "    else:\n",
    "        keys.to_csv('data/sharp_mx_'+str(start_date)+'_'+str(end_date)+'.csv', index= False, header=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#6 - data colect from JSOC - all events - flare and no-flare events\n",
    "\n",
    "start = dt.datetime.strptime(start_date, '%Y-%m-%d') \n",
    "end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "size = (end-start).days * 4\n",
    "\n",
    "all_times = {start + dt.timedelta(hours=i*6) for i in range(size)}\n",
    "all_times = list(all_times)\n",
    "all_times.sort()\n",
    "\n",
    "\n",
    "for al in all_times:\n",
    "    \n",
    "    dataformat = str(al)\n",
    "\n",
    "    c = drms.Client(server='jsoc', email='julianasabfer@gmail.com', verbose=False, debug=False)\n",
    "    keys, segments = c.query('hmi.sharp_720s[]['+dataformat+'] ', key='OBS_VR,QUALITY,DATE,DATE_S,DATE_B,DATE__OBS,DATE-OBS,T_OBS,T_REC,harpnum,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANALP,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR', seg='Br')\n",
    "\n",
    "    \n",
    "    if keys.empty:\n",
    "        print(\"there are no matching data for datetime: \", str(date_event))\n",
    "        continue\n",
    "\n",
    "    \n",
    "    keys[\"Class_Flare\"] = \"NF\"\n",
    "    keys[\"Class\"] = 0\n",
    "\n",
    "\n",
    "    if(os.path.isfile('data/sharp_no-class_'+str(start_date)+'_'+str(end_date)+'.csv')):\n",
    "        keys.to_csv('data/sharp_no-class_'+str(start_date)+'_'+str(end_date)+'.csv', mode='a', index= False,header=False)\n",
    "    else:\n",
    "        keys.to_csv('data/sharp_no-class_'+str(start_date)+'_'+str(end_date)+'.csv', index= False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7  - Set rows corresponding to solar flare events and remove rows without an active region (ARs)\n",
    "\n",
    "raw_df = pd.read_csv(f'data/sharp_no-class_{start_date}_{end_date}.csv', sep=\",\")\n",
    "raw_df['T_REC'] = pd.to_datetime(raw_df['T_REC'], format='%Y.%m.%d_%H:%M:%S_TAI')\n",
    "\n",
    "# Loop over GOES events\n",
    "for event in goes_events:\n",
    "\n",
    "    event_date = round_up(event['peak_time'].to_datetime())\n",
    "    harpnum = convert_noaa_to_harpnum(event['noaa_active_region'])\n",
    "\n",
    "    # Short window: -6h to event\n",
    "    start_time = event_date - dt.timedelta(hours=6)\n",
    "    end_time = event_date\n",
    "\n",
    "    # Filter with harp_num\n",
    "    mask_harp = (raw_df[\"T_REC\"].between(start_time, end_time)) & (raw_df[\"harpnum\"] == harpnum)\n",
    "    num_matches_harp = mask_harp.sum()\n",
    "\n",
    "    if num_matches_harp > 0:\n",
    "        raw_df.loc[mask_harp, \"Class\"] = 1\n",
    "        print(f\"[FLARE] {num_matches_harp} lines marked as Class=1 for HARP {harpnum} ({start_time} → {end_time})\")\n",
    "\n",
    "    else:\n",
    "        # Same window but without harp_num\n",
    "        mask_all = raw_df[\"T_REC\"].between(start_time, end_time)\n",
    "        num_matches_all = mask_all.sum()\n",
    "\n",
    "        if num_matches_all > 0:\n",
    "            raw_df.loc[mask_all, \"Class\"] = 2\n",
    "            print(f\"[ALT] No records for HARP {harpnum}, but {num_matches_all} lines marked as Class=2 (no harp filter) between {start_time} → {end_time}\")\n",
    "        else:\n",
    "            print(f\"[WARN] No data found even without harp filter ({start_time} → {end_time})\")\n",
    "\n",
    "\n",
    "# After loop: remove Class=2 records\n",
    "num_to_remove = (raw_df[\"Class\"] == 2).sum()\n",
    "print(f\"[INFO] Removing {num_to_remove} lines with Class=2...\")\n",
    "\n",
    "df_final = raw_df.loc[raw_df[\"Class\"] != 2].copy()\n",
    "\n",
    "print(f\"[INFO] Remaining lines after removal: {len(df_final)}\")\n",
    "\n",
    "df_final.to_csv('data/sharp_no-flare_'+str(start_date)+'_'+str(end_date)+'.csv', index= False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 create full dataset\n",
    "\n",
    "#load file solar flare events sharp\n",
    "df_events_abcmx = pd.read_csv('data/sharp_mx_'+str(start_date)+'_'+str(end_date)+'.csv', sep=\",\")\n",
    "date1 = pd.to_datetime(df_events_abcmx ['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2 = pd.to_datetime(df_events_abcmx ['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "df_events_abcmx ['T_REC'] = date1.fillna(date2)\n",
    "\n",
    "#load file no-evets sharp\n",
    "df_events_noflare = pd.read_csv('data/sharp_no-flare_'+str(start_date)+'_'+str(end_date)+'.csv', sep=\",\")\n",
    "date1 = pd.to_datetime(df_events_noflare['T_REC'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "date2 = pd.to_datetime(df_events_noflare['T_REC'], errors='coerce', format='%Y-%m-%d')\n",
    "df_events_noflare['T_REC'] = date1.fillna(date2)\n",
    "\n",
    "\n",
    "#concat events\n",
    "\n",
    "print(\"Nº ABCMX: \", len(df_events_abcmx))\n",
    "print(\"Nº NF: \", len(df_events_noflare))\n",
    "\n",
    "df_all_events = pd.concat([df_events_abcmx, df_events_noflare])\n",
    "print(\"All events: \", len(df_all_events))\n",
    "\n",
    "\n",
    "#drop duplicates rows\n",
    "df_all_events = df_all_events.drop_duplicates()\n",
    "print(\"All events after removed duplicates:\", len(df_all_events))\n",
    "\n",
    "#Discart bad rows\n",
    "mask = (df_all_events['QUALITY'] >= 65536) | (df_all_events['OBS_VR'].abs() >= 3500)\n",
    "df_all_events_discart = df_all_events.loc[~mask]\n",
    "\n",
    "print(\"All events after QUALITY and OBS_VR: \", len(df_all_events_discart))\n",
    "\n",
    "\n",
    "#Discart null rows\n",
    "df_all_events_discart.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_all_events_discart.dropna(subset=df_all_events_discart.columns, inplace=True)\n",
    "df_all_events_discart.drop\n",
    "\n",
    "print(\"All events after dicart null rows: \", len(df_all_events_discart))\n",
    "\n",
    "df_all_events.to_csv('data/sharp_mx-nf-all_'+str(start_date)+'_'+str(end_date)+'.csv', index= False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
